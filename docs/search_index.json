[
["exercise-3-3-1.html", "STAT 5361 Statistical Computing Solution Problem 1 Exercise 3.3.1 1.1 Get Fisher Information 1.2 Implement loglikelihood with a random sample and plot against \\(\\theta\\) 1.3 Newton-Raphson method 1.4 Fixed point method 1.5 Fisher scoring and Newton-Raphson 1.6 comparing the different methods", " STAT 5361 Statistical Computing Solution Disheng Mao 2018-10-03 Problem 1 Exercise 3.3.1 1.1 Get Fisher Information \\[\\begin{align*} f(x;\\theta) &amp;= \\frac{1}{\\pi(1+(x-\\theta)^2)}\\\\ \\Rightarrow l(\\theta) &amp;= \\sum_{i = 1}^n \\ln(f(X_i;\\theta)) = -n\\ln(\\pi) - \\sum_{i = 1}^n\\ln(1+(X_i-\\theta)^2)\\\\ \\Rightarrow l&#39;(\\theta) &amp;= -2\\sum_{i = 1}^n\\frac{\\theta-X_i}{1+(\\theta-X_i)^2}\\\\ \\Rightarrow l&#39;&#39;(\\theta) &amp;= -2\\sum_{i = 1}^n[\\frac{1}{1+(\\theta-X_i)^2} - \\frac{2(\\theta-X_i)^2}{[1+(\\theta - X_i)^2]^2}] = -2\\sum_{i = 1}^n\\frac{1-(\\theta-X_1)^2}{[1+(\\theta-X_i)^2]^2}\\\\ \\Rightarrow I_n(\\theta) &amp;= -El&#39;&#39;(\\theta)\\\\ &amp;= 2nE\\frac{1-(\\theta - X)^2}{[1+(\\theta-X)^2]^2}\\\\ &amp;=\\frac{2n}{\\pi}\\int_R\\frac{1-(x-\\theta)^2}{(1+(x-\\theta)^2)^3}dx\\\\ &amp;=\\frac{2n}{\\pi}\\int_R\\frac{1-x^2}{(1+x^2)^3}dx\\\\ &amp;= \\frac{2n}{\\pi}\\int_R\\frac{-1}{(1+x^2)^2}+2\\frac{2}{(1+x^2)^3}dx\\\\ \\end{align*}\\] Also: \\[\\begin{align*} M_k &amp;= \\int_R\\frac{1}{(1+x^2)^k}dx\\\\ &amp;= \\int_R\\frac{1+x^2}{(1+x^2)^{k+1}}dx\\\\ &amp;= M_{k+1} + \\int_R\\frac{x^2}{(1+x^2)^{k+1}}dx\\\\ &amp;= M_{k+1} + \\int_R\\frac{2kx}{(1+x^2)^{k+1}}\\frac{x}{2k}dx = M_{k+1}+\\frac{1}{2k}M_k\\\\ \\end{align*}\\] Since \\(M_1 = \\pi\\), we have \\(M_2 = \\pi/2, M_3 = 3\\pi/8\\), then \\(I_n(\\theta) = n/2\\). 1.2 Implement loglikelihood with a random sample and plot against \\(\\theta\\) Use the loglikelihood function we got from above, set n = 10 and plug in the generated sample value \\(X_i\\), we can get the loglikelihood function. When generating sample, the location parameter was set to be \\(\\theta = 5\\). The loglikelihood function curve against \\(\\theta\\) are shown in Figure : set.seed(20180909) n &lt;- 10 X &lt;- rcauchy(n, location = 5, scale = 1) loglik.0 &lt;- function(theta) { l &lt;- sum(dcauchy(X, location = theta, log = TRUE)) l } loglik &lt;- function(theta) { l &lt;- sapply(theta, FUN = loglik.0) l } curve(loglik, -10, 10) 1.3 Newton-Raphson method library(pracma) ## define the derivitive function dev.loglik &lt;- function(theta) { dev.l &lt;- -2 * sum((theta-X)/(1+(theta-X)^2)) dev.l } ## define the hessian function hessian.loglik &lt;- function(theta) { h &lt;- -2 * sum((1-(theta-X)^2) * (1+(theta-X)^2)^(-2)) h } x0 &lt;- seq(-10, 20, by = 0.5) root.newton &lt;- rep(0, length(x0)) for (i in 1:length(x0)) { root.newton[i] &lt;- newtonRaphson(dev.loglik, x0 = x0[i], dfun = hessian.loglik)$root } plot(x0, root.newton) abline(h = 5.442) root.newton ## [1] -4.324741e+31 -4.193577e+31 -4.062249e+31 -3.930748e+31 -3.799064e+31 ## [6] -3.667185e+31 -3.535100e+31 -3.402796e+31 -3.270261e+31 -3.137479e+31 ## [11] -3.004436e+31 -2.871118e+31 -2.737510e+31 -2.603599e+31 -2.469374e+31 ## [16] -2.334832e+31 -2.199981e+31 -2.064850e+31 -1.929508e+31 -1.794100e+31 ## [21] -1.658922e+31 -1.524582e+31 -1.392396e+31 -1.265439e+31 -1.151924e+31 ## [26] -1.079358e+31 -1.199750e+31 1.502957e+32 2.056366e+01 2.108229e+01 ## [31] 5.685422e+00 5.685422e+00 5.685422e+00 5.685422e+00 3.215974e+31 ## [36] -9.558888e+30 1.937744e+01 2.108229e+01 5.685422e+00 -8.759488e+30 ## [41] 2.108229e+01 5.685422e+00 7.439560e+30 4.429077e+31 2.056366e+01 ## [46] 2.056366e+01 2.056366e+01 2.108229e+01 2.108229e+01 2.108230e+01 ## [51] 1.937743e+01 2.056366e+01 2.056366e+01 1.937744e+01 1.937743e+01 ## [56] 1.937744e+01 -2.825479e+30 2.056366e+01 2.056366e+01 2.056366e+01 ## [61] 2.056366e+01 We can see that Newton method doesnâ€™t converge when initial value is not close to the real root. 1.4 Fixed point method ## self-defined fixed point methods to find mle ## input gradiant of loglikelihood function, x0 is initial value FixPoint.mle &lt;- function(dev.loglik, alpha, x0, maxiter = 100, tol = .Machine$double.eps^0.5){ x &lt;- x0 for (i in 1:maxiter) { x.new &lt;- alpha * dev.loglik(x) + x if (abs(x.new - x) &lt; tol) break x &lt;- x.new } if (i == maxiter) warning(&quot;maximum iteration has reached&quot;) return(list(root = x, niter = i)) } alpha &lt;- c(1, 0.64, 0.25) root.fixpoint &lt;- matrix(0, ncol = length(alpha), nrow = length(x0)) for (i in 1:length(alpha)) { for (j in 1:length(x0)) { root.fixpoint[j, i] &lt;- FixPoint.mle(dev.loglik = dev.loglik, alpha = alpha[i], x0 = x0[j])$root } } plot(x0, root.fixpoint[, 1], ylim = c(min(root.fixpoint), max(root.fixpoint)), ylab = &quot;root&quot;, xlab = &quot;initial value&quot;, main = paste0(&quot;black: &quot;, expression(alpha), &quot;= 1; red: &quot;, expression(alpha), &quot;= 0.64; green: &quot;, expression(alpha), &quot;= 0.25&quot;)) points(x0, root.fixpoint[, 2], col = &quot;red&quot;) points(x0, root.fixpoint[, 3], col = &quot;green&quot;) 1.5 Fisher scoring and Newton-Raphson ## Self-defined fisher scoring method to find mle. ## input gradiant of loglikelihood and sample fisher information. FisherScore.mle &lt;- function(dev.loglik, information, x0, maxiter = 100, tol = .Machine$double.eps^0.5) { x &lt;- x0 for (i in 1:maxiter) { x.new &lt;- x + dev.loglik(x) / information(x) if (abs(x.new - x) &lt; tol) break x &lt;- x.new } if (i == maxiter) warning(&quot;maximum iteration has reached&quot;) return(list(root = x, niter = i)) } FisherNewton.mle &lt;- function(dev.loglik, information, dfun, x0, maxiter = 100, tol = .Machine$double.eps^0.5) { method.fisher &lt;- FisherScore.mle(dev.loglik = dev.loglik, information = information, x0 = x0, maxiter = maxiter, tol = tol) x.fisher &lt;- method.fisher$root niter.fisher &lt;- method.fisher$niter method.newton &lt;- newtonRaphson(fun = dev.loglik, x0 = x.fisher, dfun = dfun, maxiter = maxiter, tol = tol) return(list(root = method.newton$root, niter.fisher = niter.fisher, niter.newton = method.newton$niter)) } inf.cauchy &lt;- function(x) n/2 root.mix &lt;- rep(0, length(x0)) for (i in 1:length(x0)) { root.mix[i] &lt;- FisherNewton.mle(dev.loglik, inf.cauchy, dfun = hessian.loglik, x0 = x0[i])$root } plot(x0, root.mix) 1.6 comparing the different methods library(microbenchmark) ## comparing the speed of different methods ## use starting point 5, alpha = 0.25 for fixed point method newton.method &lt;- newtonRaphson(fun = dev.loglik, x0 = 5, dfun = hessian.loglik) fixpoint.method &lt;- FixPoint.mle(dev.loglik = dev.loglik, alpha = 0.25, x0 = 5) fishernewton.method &lt;- FisherNewton.mle(dev.loglik = dev.loglik, information = inf.cauchy, dfun = hessian.loglik, x0 = 5) list(newton.niter = newton.method$niter, fixpoint.niter = fixpoint.method$niter, fishernewton.niter = c(fishernewton.method$niter.fisher, fishernewton.method$niter.newton)) ## $newton.niter ## [1] 6 ## ## $fixpoint.niter ## [1] 17 ## ## $fishernewton.niter ## [1] 8 1 Fixed point method is most stable but converges slowly compare to the other two methods. Newton-Raphson methods converges fastest but is the most unstably one. Fisher-Scoring converges slower than Newton, but is very stable and accuracy, after refining with Newton-Raphson methods. Also we can see that if we use fisher scoring root to be the initial value of Newton-Raphson method, it will converge very fast. "],
["exercise-3-3-2.html", "Problem 2 Exercise 3.3.2", " Problem 2 Exercise 3.3.2 ##&#39; define the loglikelihood function loglik.my0 &lt;- function(theta, sample) { n &lt;- length(sample) if (sum(sample &gt;=0 &amp; sample &lt;= 2*pi) &lt; n) { print(&quot;sample is out of range&quot;) } else if(theta &lt; -pi | theta &gt; pi) { print(&quot;theta is out of range&quot;) } else { l &lt;- sum(log(1-cos(sample-theta))) - n * log(2*pi) return(l) } } loglik.my &lt;- function(theta, sample) { l &lt;- sapply(theta, FUN = loglik.my0, sample = sample) l } s &lt;- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52) curve(loglik.my(x, sample = s), -pi, pi) \\[\\begin{align*} E(X|\\theta) &amp;= \\int_0^{2\\pi}x\\frac{1-\\cos(x-\\theta)}{2\\pi}dx\\\\ &amp;= \\frac{1}{2\\pi} \\int_0^{2\\pi}xdx - \\frac{1}{2\\pi}\\int_0^{2\\pi} x\\cos(x-\\theta)dx\\\\ &amp;= \\left.\\frac{1}{2\\pi} \\times \\frac{1}{2}\\right\\vert_0^{2\\pi} - \\frac{1}{2\\pi}\\int_0^{2\\pi}xd\\sin(x-\\theta)\\\\ &amp;= \\pi - \\frac{1}{2\\pi} \\left[\\left.x\\sin(x-\\theta)\\right\\vert_0^ {2\\pi}-\\int_0^{2\\pi}\\sin(x-\\theta)dx\\right]\\\\ &amp;= \\pi - \\frac{1}{2\\pi} \\left[2\\pi\\sin(2\\pi-\\theta)+\\cos(x-\\theta) \\left.\\right\\vert_0^{2\\pi}\\right]\\\\ &amp;= \\pi - \\frac{1}{2\\pi} \\left[-2\\pi\\sin(\\theta)\\right]\\\\ &amp;= \\pi + \\sin(\\theta)\\\\ &amp;= \\bar{X}_n\\\\ \\end{align*}\\] \\[\\Rightarrow\\tilde{\\theta}_n = \\arcsin(\\bar{X}_n - \\pi)\\] library(pracma) theta_0 &lt;- asin(mean(s) - pi) ##&#39; define derivitive of log-likelihood function dev.loglik0 &lt;- function(theta, sample) { dev.l &lt;- sum(sin(theta-sample)/(1-cos(theta-sample))) dev.l } dev.loglik &lt;- function(theta, sample) { dev.l &lt;- sapply(theta, FUN = dev.loglik0, sample = sample) } x1 &lt;- newtonRaphson(fun = dev.loglik, x0 = theta_0, sample = s)$root x2 &lt;- newtonRaphson(fun = dev.loglik, x0 = -2.7, sample = s)$root x3 &lt;- newtonRaphson(fun = dev.loglik, x0 = 2.7, sample = s)$root x1 ## [1] 0.003118157 x2 ## [1] -2.668857 x3 ## [1] 2.848415 Newton-Raphson method gives 0.0031 as MLE when MOM 0.095 is initial value, -2.669 as MLE when -2.7 is initial value, 2.848 as MLE when 2.7 is initial value. n &lt;- 200 init &lt;- seq(-pi, pi, length.out = n) root.newton &lt;- rep(0, n) for (i in 1:n) root.newton[i] &lt;- newtonRaphson(fun = dev.loglik, x0 = init[i], sample = s)$root plot(init, root.newton, type = &quot;l&quot;, xlab = &quot;initial value&quot;) "]
]
