[
["exercise-3-1.html", "STAT 5361 Statistical Computing Solution Problem 1 Exercise 3.1 1.1 Get Fisher Information 1.2 Implement loglikelihood with a random sample and plot against \\(\\theta\\) 1.3 Newton-Raphson method 1.4 Fixed point method 1.5 Fisher scoring and Newton-Raphson 1.6 comparing the different methods", " STAT 5361 Statistical Computing Solution 2018-10-16 Problem 1 Exercise 3.1 1.1 Get Fisher Information \\[\\begin{align*} f(x;\\theta) &amp;= \\frac{1}{\\pi(1+(x-\\theta)^2)}\\\\ \\Rightarrow l(\\theta) &amp;= \\sum_{i = 1}^n \\ln(f(X_i;\\theta)) = -n\\ln(\\pi) - \\sum_{i = 1}^n\\ln(1+(X_i-\\theta)^2)\\\\ \\Rightarrow l&#39;(\\theta) &amp;= -2\\sum_{i = 1}^n\\frac{\\theta-X_i}{1+(\\theta-X_i)^2}\\\\ \\Rightarrow l&#39;&#39;(\\theta) &amp;= -2\\sum_{i = 1}^n[\\frac{1}{1+(\\theta-X_i)^2} - \\frac{2(\\theta-X_i)^2}{[1+(\\theta - X_i)^2]^2}] = -2\\sum_{i = 1}^n\\frac{1-(\\theta-X_1)^2}{[1+(\\theta-X_i)^2]^2}\\\\ \\Rightarrow I_n(\\theta) &amp;= -El&#39;&#39;(\\theta)\\\\ &amp;= 2nE\\frac{1-(\\theta - X)^2}{[1+(\\theta-X)^2]^2}\\\\ &amp;=\\frac{2n}{\\pi}\\int_R\\frac{1-(x-\\theta)^2}{(1+(x-\\theta)^2)^3}dx\\\\ &amp;=\\frac{2n}{\\pi}\\int_R\\frac{1-x^2}{(1+x^2)^3}dx\\\\ &amp;= \\frac{2n}{\\pi}\\int_R\\frac{-1}{(1+x^2)^2}+2\\frac{2}{(1+x^2)^3}dx\\\\ \\end{align*}\\] Also: \\[\\begin{align*} M_k &amp;= \\int_R\\frac{1}{(1+x^2)^k}dx\\\\ &amp;= \\int_R\\frac{1+x^2}{(1+x^2)^{k+1}}dx\\\\ &amp;= M_{k+1} + \\int_R\\frac{x^2}{(1+x^2)^{k+1}}dx\\\\ &amp;= M_{k+1} + \\int_R\\frac{2kx}{(1+x^2)^{k+1}}\\frac{x}{2k}dx = M_{k+1}+\\frac{1}{2k}M_k\\\\ \\end{align*}\\] Since \\(M_1 = \\pi\\), we have \\(M_2 = \\pi/2, M_3 = 3\\pi/8\\), then \\(I_n(\\theta) = n/2\\). 1.2 Implement loglikelihood with a random sample and plot against \\(\\theta\\) Use the loglikelihood function we got from above, set n = 10 and plug in the generated sample value \\(X_i\\), we can get the loglikelihood function. When generating sample, the location parameter was set to be \\(\\theta = 5\\). The loglikelihood function curve against \\(\\theta\\) are shown in Figure : set.seed(20180909) n &lt;- 10 X &lt;- rcauchy(n, location = 5, scale = 1) loglik.0 &lt;- function(theta) { l &lt;- sum(dcauchy(X, location = theta, log = TRUE)) l } loglik &lt;- function(theta) { l &lt;- sapply(theta, FUN = loglik.0) l } curve(loglik, -10, 10) 1.3 Newton-Raphson method library(pracma) ## define the derivitive function dev.loglik &lt;- function(theta) { dev.l &lt;- -2 * sum((theta-X)/(1+(theta-X)^2)) dev.l } ## define the hessian function hessian.loglik &lt;- function(theta) { h &lt;- -2 * sum((1-(theta-X)^2) * (1+(theta-X)^2)^(-2)) h } x0 &lt;- seq(-10, 20, by = 0.5) root.newton &lt;- rep(0, length(x0)) for (i in 1:length(x0)) { root.newton[i] &lt;- newtonRaphson(dev.loglik, x0 = x0[i], dfun = hessian.loglik)$root } plot(x0, root.newton) abline(h = 5.442) root.newton ## [1] -4.324741e+31 -4.193577e+31 -4.062249e+31 -3.930748e+31 -3.799064e+31 ## [6] -3.667185e+31 -3.535100e+31 -3.402796e+31 -3.270261e+31 -3.137479e+31 ## [11] -3.004436e+31 -2.871118e+31 -2.737510e+31 -2.603599e+31 -2.469374e+31 ## [16] -2.334832e+31 -2.199981e+31 -2.064850e+31 -1.929508e+31 -1.794100e+31 ## [21] -1.658922e+31 -1.524582e+31 -1.392396e+31 -1.265439e+31 -1.151924e+31 ## [26] -1.079358e+31 -1.199750e+31 1.502957e+32 2.056366e+01 2.108229e+01 ## [31] 5.685422e+00 5.685422e+00 5.685422e+00 5.685422e+00 3.215974e+31 ## [36] -9.558888e+30 1.937744e+01 2.108229e+01 5.685422e+00 -8.759488e+30 ## [41] 2.108229e+01 5.685422e+00 7.439560e+30 4.429077e+31 2.056366e+01 ## [46] 2.056366e+01 2.056366e+01 2.108229e+01 2.108229e+01 2.108230e+01 ## [51] 1.937743e+01 2.056366e+01 2.056366e+01 1.937744e+01 1.937743e+01 ## [56] 1.937744e+01 -2.825479e+30 2.056366e+01 2.056366e+01 2.056366e+01 ## [61] 2.056366e+01 We can see that Newton method doesn’t converge when initial value is not close to the real root. 1.4 Fixed point method ## self-defined fixed point methods to find mle ## input gradiant of loglikelihood function, x0 is initial value FixPoint.mle &lt;- function(dev.loglik, alpha, x0, maxiter = 100, tol = .Machine$double.eps^0.5){ x &lt;- x0 for (i in 1:maxiter) { x.new &lt;- alpha * dev.loglik(x) + x if (abs(x.new - x) &lt; tol) break x &lt;- x.new } if (i == maxiter) warning(&quot;maximum iteration has reached&quot;) return(list(root = x, niter = i)) } alpha &lt;- c(1, 0.64, 0.25) root.fixpoint &lt;- matrix(0, ncol = length(alpha), nrow = length(x0)) for (i in 1:length(alpha)) { for (j in 1:length(x0)) { root.fixpoint[j, i] &lt;- FixPoint.mle(dev.loglik = dev.loglik, alpha = alpha[i], x0 = x0[j])$root } } plot(x0, root.fixpoint[, 1], ylim = c(min(root.fixpoint), max(root.fixpoint)), ylab = &quot;root&quot;, xlab = &quot;initial value&quot;, main = paste0(&quot;black: &quot;, expression(alpha), &quot;= 1; red: &quot;, expression(alpha), &quot;= 0.64; green: &quot;, expression(alpha), &quot;= 0.25&quot;)) points(x0, root.fixpoint[, 2], col = &quot;red&quot;) points(x0, root.fixpoint[, 3], col = &quot;green&quot;) 1.5 Fisher scoring and Newton-Raphson ## Self-defined fisher scoring method to find mle. ## input gradiant of loglikelihood and sample fisher information. FisherScore.mle &lt;- function(dev.loglik, information, x0, maxiter = 100, tol = .Machine$double.eps^0.5) { x &lt;- x0 for (i in 1:maxiter) { x.new &lt;- x + dev.loglik(x) / information(x) if (abs(x.new - x) &lt; tol) break x &lt;- x.new } if (i == maxiter) warning(&quot;maximum iteration has reached&quot;) return(list(root = x, niter = i)) } FisherNewton.mle &lt;- function(dev.loglik, information, dfun, x0, maxiter = 100, tol = .Machine$double.eps^0.5) { method.fisher &lt;- FisherScore.mle(dev.loglik = dev.loglik, information = information, x0 = x0, maxiter = maxiter, tol = tol) x.fisher &lt;- method.fisher$root niter.fisher &lt;- method.fisher$niter method.newton &lt;- newtonRaphson(fun = dev.loglik, x0 = x.fisher, dfun = dfun, maxiter = maxiter, tol = tol) return(list(root = method.newton$root, niter.fisher = niter.fisher, niter.newton = method.newton$niter)) } inf.cauchy &lt;- function(x) n/2 root.mix &lt;- rep(0, length(x0)) for (i in 1:length(x0)) { root.mix[i] &lt;- FisherNewton.mle(dev.loglik, inf.cauchy, dfun = hessian.loglik, x0 = x0[i])$root } plot(x0, root.mix) 1.6 comparing the different methods library(microbenchmark) ## comparing the speed of different methods ## use starting point 5, alpha = 0.25 for fixed point method newton.method &lt;- newtonRaphson(fun = dev.loglik, x0 = 5, dfun = hessian.loglik) fixpoint.method &lt;- FixPoint.mle(dev.loglik = dev.loglik, alpha = 0.25, x0 = 5) fishernewton.method &lt;- FisherNewton.mle(dev.loglik = dev.loglik, information = inf.cauchy, dfun = hessian.loglik, x0 = 5) list(newton.niter = newton.method$niter, fixpoint.niter = fixpoint.method$niter, fishernewton.niter = c(fishernewton.method$niter.fisher, fishernewton.method$niter.newton)) ## $newton.niter ## [1] 6 ## ## $fixpoint.niter ## [1] 17 ## ## $fishernewton.niter ## [1] 8 1 Fixed point method is most stable but converges slowly compare to the other two methods. Newton-Raphson methods converges fastest but is the most unstably one. Fisher-Scoring converges slower than Newton, but is very stable and accuracy, after refining with Newton-Raphson methods. Also we can see that if we use fisher scoring root to be the initial value of Newton-Raphson method, it will converge very fast. "],
["exercise-3-2.html", "Problem 2 Exercise 3.2 2.1 Find the the log-likelihood function 2.2 Find the method-of-moments estimator 2.3 Find the MLE 2.4 \\(\\theta_0 = 2.7\\) or \\(\\theta_0 = -2.7\\) 2.5 Repeat the above using 200 equally spaced starting values", " Problem 2 Exercise 3.2 2.1 Find the the log-likelihood function The log-likelihood function of this distribution is \\[ \\ell(\\mathbf{x}, \\theta) = \\sum_{i=1}^n \\log\\{1-\\cos(x_i-\\theta)\\} - n\\log2\\pi\\] x &lt;- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52) loglikelihood &lt;- function(theta){ n &lt;- length(x) s &lt;- sum(log(1 - cos(x - theta))) + n * log(2 * pi) return(s) } loglikelihood &lt;- Vectorize(loglikelihood) curve(loglikelihood, -pi, pi, xlab = expression(theta), ylab = &quot;log-likelihood&quot;) 2.2 Find the method-of-moments estimator The expectation of \\(\\mathbf{x}|\\theta\\) is \\[\\begin{align*} \\mathbb E (x | \\theta) &amp;= \\int_{0}^{2\\pi} x \\frac{1-\\cos(x-\\theta)}{2\\pi} dx \\\\ &amp;= \\frac{1}{2\\pi} \\int_{0}^{2\\pi} x - x\\cos(x-\\theta) dx \\\\ &amp;= \\pi + \\sin(\\theta) \\\\ &amp;= \\bar{X_n} \\end{align*}\\] Thus, theta_tilde &lt;- asin(mean(x) - pi) theta_tilde ## [1] 0.09539407 2.3 Find the MLE Since \\[\\frac{\\partial\\ell(\\mathbf{x}; \\theta)}{\\partial\\theta} = \\sum_{i=1}^n \\frac{-\\sin(x_i - \\theta)}{1-\\cos(x_i-\\theta)}\\] \\[\\frac{\\partial^2\\ell(\\mathbf{x}; \\theta)}{\\partial \\theta^2} = \\sum_{i=1}^n \\frac{\\cos(x_i-\\theta) - \\cos^2(x_i-\\theta)-\\sin^2(x_i-\\theta)}{(1-\\cos(x_i-\\theta))^2}\\] The Newton-Raphson method is \\[\\hat\\theta^{(t+1)} = \\hat\\theta^{(t)} - \\left\\{\\frac{\\partial^2\\ell(\\mathbf{x}; \\hat\\theta^{(t)})}{\\partial \\theta^2}\\right\\}^{-1}\\frac{\\partial\\ell(\\mathbf{x}; \\hat\\theta^{(t)})}{\\partial\\theta}\\] lfd &lt;- function(theta){ sum(-sin(x-theta)/(1-cos(x-theta))) } lsd &lt;- function(theta){ sum((cos(x-theta) - (cos(x-theta))^2 - (sin(x-theta))^2)/(1-cos(x-theta))^2) } Newton &lt;- function(init){ theta0 &lt;- init i &lt;- 0 diff &lt;- 1 msg &lt;- &quot;converge&quot; while(abs(diff) &gt; 0.0000001){ lfd &lt;- lfd(theta0) lsd &lt;- lsd(theta0) diff &lt;- (lfd/lsd) theta1 &lt;- theta0 - diff theta0 &lt;- theta1 i &lt;- i+1 #cat(i) if(i &gt;= 150){ msg &lt;- &quot;Not converge&quot; theta0 &lt;- Inf break } } return(list(theta = theta0, itr = i, msg = msg)) } Newton(theta_tilde) ## $theta ## [1] 0.003118157 ## ## $itr ## [1] 4 ## ## $msg ## [1] &quot;converge&quot; 2.4 \\(\\theta_0 = 2.7\\) or \\(\\theta_0 = -2.7\\) Newton(-2.7) ## $theta ## [1] -2.668857 ## ## $itr ## [1] 4 ## ## $msg ## [1] &quot;converge&quot; Newton(2.7) ## $theta ## [1] 2.848415 ## ## $itr ## [1] 5 ## ## $msg ## [1] &quot;converge&quot; The \\(\\hat\\theta\\) we got is different. 2.5 Repeat the above using 200 equally spaced starting values init &lt;- seq(-pi, pi, length.out=200) result &lt;- NULL for(initi in init){ result &lt;- rbind(result, c(initi, Newton(initi)$theta)) } colnames(result) &lt;- c(&quot;Initial_value&quot;, &quot;theta_hat&quot;) split(result, result[,2]) ## $`-3.11247050669846` ## [1] -3.141593 -3.110019 -3.078445 -3.046871 -3.015297 -2.983724 -2.952150 ## [8] -2.920576 -2.889002 -2.857428 -2.825855 -3.112471 -3.112471 -3.112471 ## [15] -3.112471 -3.112471 -3.112471 -3.112471 -3.112471 -3.112471 -3.112471 ## [22] -3.112471 ## ## $`-2.78655685241805` ## [1] -2.794281 -2.786557 ## ## $`-2.78655685241804` ## [1] -2.762707 -2.786557 ## ## $`-2.66885745902142` ## [1] -2.731133 -2.699560 -2.667986 -2.636412 -2.604838 -2.668857 -2.668857 ## [8] -2.668857 -2.668857 -2.668857 ## ## $`-2.50935603320277` ## [1] -2.573264 -2.541691 -2.510117 -2.478543 -2.446969 -2.415395 -2.509356 ## [8] -2.509356 -2.509356 -2.509356 -2.509356 -2.509356 ## ## $`-2.38826662826452` ## [1] -2.383822 -2.388267 ## ## $`-2.29792596896698` ## [1] -2.352248 -2.297926 ## ## $`-2.29792596896697` ## [1] -2.320674 -2.289100 -2.257526 -2.297926 -2.297926 -2.297926 ## ## $`-2.23219189887219` ## [1] -2.225953 -2.232192 ## ## $`-1.66271239546243` ## [1] -2.194379 -2.162805 -2.131231 -2.099657 -2.068084 -2.036510 -2.004936 ## [8] -1.973362 -1.941788 -1.910215 -1.878641 -1.847067 -1.815493 -1.783919 ## [15] -1.752346 -1.720772 -1.689198 -1.594477 -1.531329 -1.499755 -1.468181 ## [22] -1.662712 -1.662712 -1.662712 -1.662712 -1.662712 -1.662712 -1.662712 ## [29] -1.662712 -1.662712 -1.662712 -1.662712 -1.662712 -1.662712 -1.662712 ## [36] -1.662712 -1.662712 -1.662712 -1.662712 -1.662712 -1.662712 -1.662712 ## ## $`-1.66271239546242` ## [1] -1.657624 -1.626050 -1.562903 -1.662712 -1.662712 -1.662712 ## ## $`-1.44750255268373` ## [1] -1.436608 -1.447503 ## ## $`-0.95440583712848` ## [1] -1.4050339 -1.3103125 -1.2787387 -1.2155911 -1.1208697 -1.0577221 ## [7] -1.0261484 -0.9945746 -0.9544058 -0.9544058 -0.9544058 -0.9544058 ## [13] -0.9544058 -0.9544058 -0.9544058 -0.9544058 ## ## $`-0.954405837128479` ## [1] -1.3734601 -1.3418863 -1.1524435 -1.0892959 -0.9630008 -0.8998532 ## [7] -0.8682794 -0.8367056 -0.9544058 -0.9544058 -0.9544058 -0.9544058 ## [13] -0.9544058 -0.9544058 -0.9544058 -0.9544058 ## ## $`-0.954405837128476` ## [1] -0.9314270 -0.9544058 ## ## $`-0.95440583712847` ## [1] -1.2471649 -0.9544058 ## ## $`-0.954405837128466` ## [1] -1.1840173 -0.9544058 ## ## $`0.00311815708656577` ## [1] -0.489393830 0.003118157 ## ## $`0.0031181570865658` ## [1] -0.078934489 0.003118157 ## ## $`0.00311815708656581` ## [1] 0.110508284 0.003118157 ## ## $`0.00311815708656585` ## [1] -0.236803466 0.003118157 ## ## $`0.00311815708656587` ## [1] -0.142082080 -0.047360693 0.003118157 0.003118157 ## ## $`0.00311815708656589` ## [1] -0.678836604 -0.584115217 0.003118157 0.003118157 ## ## $`0.00311815708656591` ## [1] -0.110508284 0.015786898 0.003118157 0.003118157 ## ## $`0.00311815708656593` ## [1] -0.710410399 0.142082080 0.299951057 0.003118157 0.003118157 ## [6] 0.003118157 ## ## $`0.00311815708656597` ## [1] -0.457820035 0.003118157 ## ## $`0.00311815708656598` ## [1] -0.015786898 0.236803466 0.268377262 0.003118157 0.003118157 ## [6] 0.003118157 ## ## $`0.00311815708656599` ## [1] -0.805131786 0.003118157 ## ## $`0.003118157086566` ## [1] -0.741984195 0.003118157 ## ## $`0.00311815708656601` ## [1] -0.268377262 0.394672444 0.003118157 0.003118157 ## ## $`0.00311815708656602` ## [1] -0.647262808 -0.552541421 0.078934489 0.003118157 0.003118157 ## [6] 0.003118157 ## ## $`0.00311815708656603` ## [1] -0.773557990 -0.615689013 0.047360693 0.489393830 0.003118157 ## [6] 0.003118157 0.003118157 0.003118157 ## ## $`0.00311815708656604` ## [1] -0.363098648 0.003118157 ## ## $`0.00311815708656606` ## [1] 0.457820035 0.003118157 ## ## $`0.00311815708656607` ## [1] -0.426246239 0.003118157 ## ## $`0.00311815708656609` ## [1] -0.173655875 0.003118157 ## ## $`0.00311815708656611` ## [1] -0.205229671 0.003118157 ## ## $`0.00311815708656612` ## [1] -0.394672444 0.363098648 0.003118157 0.003118157 ## ## $`0.00311815708656613` ## [1] -0.299951057 0.003118157 ## ## $`0.00311815708656615` ## [1] 0.205229671 0.003118157 ## ## $`0.00311815708656793` ## [1] 0.426246239 0.003118157 ## ## $`0.00311815708656861` ## [1] -0.520967626 0.003118157 ## ## $`0.00311815708656864` ## [1] 0.331524853 0.003118157 ## ## $`0.00311815708656926` ## [1] -0.331524853 0.003118157 ## ## $`0.00311815708656987` ## [1] 0.173655875 0.003118157 ## ## $`0.812637416717926` ## [1] 1.2787387 0.8126374 ## ## $`0.812637416717938` ## [1] 0.8051318 1.4050339 0.8126374 0.8126374 ## ## $`0.812637416717939` ## [1] 0.6788366 0.8126374 ## ## $`0.81263741671794` ## [1] 0.5209676 0.5525414 0.5841152 0.6156890 0.6472628 0.7104104 0.7419842 ## [8] 0.7735580 0.8367056 0.8682794 0.8998532 0.9314270 0.9630008 0.9945746 ## [15] 1.0261484 1.0577221 1.0892959 1.1208697 1.1524435 1.1840173 1.2155911 ## [22] 1.2471649 1.3103125 1.3418863 1.3734601 1.4366077 1.4681815 1.4997553 ## [29] 1.5313291 1.5629029 1.5944767 1.6260505 1.6576243 1.6891981 1.7207719 ## [36] 1.7523457 1.7839194 1.8154932 1.8470670 1.8786408 1.9102146 1.9417884 ## [43] 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 ## [50] 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 ## [57] 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 ## [64] 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 ## [71] 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 ## [78] 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 0.8126374 ## ## $`2.00722323801594` ## [1] 1.973362 2.004936 2.068084 2.099657 2.131231 2.162805 2.194379 ## [8] 2.007223 2.007223 2.007223 2.007223 2.007223 2.007223 2.007223 ## ## $`2.00722323801595` ## [1] 2.036510 2.007223 ## ## $`2.23701292270577` ## [1] 2.225953 2.257526 2.237013 2.237013 ## ## $`2.37471166606864` ## [1] 2.289100 2.320674 2.352248 2.383822 2.415395 2.446969 2.374712 ## [8] 2.374712 2.374712 2.374712 2.374712 2.374712 ## ## $`2.48844965088485` ## [1] 2.478543 2.488450 ## ## $`2.48844965088489` ## [1] 2.510117 2.488450 ## ## $`2.84841532545741` ## [1] 2.541691 2.573264 2.604838 2.636412 2.667986 2.699560 2.731133 ## [8] 2.762707 2.794281 2.825855 2.857428 2.920576 2.952150 2.983724 ## [15] 2.848415 2.848415 2.848415 2.848415 2.848415 2.848415 2.848415 ## [22] 2.848415 2.848415 2.848415 2.848415 2.848415 2.848415 2.848415 ## ## $`2.84841532545742` ## [1] 2.889002 2.848415 ## ## $`3.17071480048113` ## [1] 3.015297 3.046871 3.078445 3.110019 3.141593 3.170715 3.170715 ## [8] 3.170715 3.170715 3.170715 "],
["exercise-3-3.html", "Problem 3 Exercise 3.3 3.1 Fit the population growth model to the beetles data using the Gauss-Newton approach 3.2 Log normal assumption", " Problem 3 Exercise 3.3 3.1 Fit the population growth model to the beetles data using the Gauss-Newton approach library(graphics) library(Matrix) ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:pracma&#39;: ## ## expm, lu, tril, triu beetles &lt;- data.frame( days = c(0, 8, 28, 41, 63, 69, 97, 117, 135, 154), beetles = c(2, 47, 192, 256, 768, 896, 1120, 896, 1184, 1024)) ##&#39; define the sum of squared errors function sqerr &lt;- function(k, r) { s &lt;- matrix(0, nrow = length(k), ncol = length(r)) for (i in 1:length(k)) { for (j in 1:length(r)) { s[i, j] &lt;- sum((beetles$beetles - k[i] * beetles$beetles[1] / (beetles$beetles[1] + (k[i] - beetles$beetles[1]) * exp(-r[j]*beetles$days)))^2) } } s } ##&#39; define z function z.vec &lt;- function(k, r) { n &lt;- length(beetles$days) z &lt;- rep(0, n) for (i in 1:n) { z[i] &lt;- beetles$beetles[i] - k*beetles$beetles[1] / (beetles$beetles[1] + (k - beetles$beetles[1])*exp(-r*beetles$days[i])) } return(z) } ##&#39; define A matrix A.mat &lt;- function(k, r) { n &lt;- length(beetles$days) A &lt;- matrix(0, nrow = n, ncol = 2) for (i in 1:n) { A[i, 1] &lt;- beetles$beetles[1]^2 * (1-exp(-r*beetles$days[i])) / (beetles$beetles[1] + (k-beetles$beetles[1])*exp(-r*beetles$days[i]))^2 A[i, 2] &lt;- beetles$beetles[1]*k*beetles$days[i]*(k-beetles$beetles[1]) * exp(-r*beetles$days[i]) / (beetles$beetles[1]+(k-beetles$beetles[1]) * exp(-r*beetles$days[i]))^2 } return(A) } gaussNewton.beetles &lt;- function(para0, z.vec, A.mat, maxiter = 100, tol = .Machine$double.eps^0.5) { para &lt;- para0 for (i in 1:maxiter) { Amat &lt;- A.mat(para[1], para[2]) zvec &lt;- z.vec(para[1], para[2]) para.new &lt;- para + solve(t(Amat) %*% Amat + 0.0001*diag(nrow = 2)) %*% t(Amat) %*% zvec if (sum(abs(para.new - para)) &lt; tol) break para &lt;- para.new } if (i == maxiter) warning(&quot;maximum iteration has reached&quot;) return(list(root = para, niter = i)) } fit &lt;- gaussNewton.beetles(para0 = c(1200, 0.1), z.vec = z.vec, A.mat = A.mat) fit$root ## [,1] ## [1,] 1049.4072443 ## [2,] 0.1182684 k &lt;- seq(100, 1500, by = 10) r &lt;- seq(0.1, 0.5, by = 0.001) z &lt;- sqerr(k, r) contour(k, r, z, xlab = &quot;k&quot;, ylab = &quot;r&quot;, main = &quot;contour plot for squared error&quot;) Estimation using Gauss-Newton method is \\(\\hat{k} = 1049\\), \\(\\hat{r} = 0.12\\). 3.2 Log normal assumption ##&#39; define log-likelihood function loglikeli &lt;- function(para) { k &lt;- para[1] r &lt;- para[2] sigma2 &lt;- para[3] l &lt;- sum(-log(2*pi*sigma2)/2 - (log(beetles$beetles) - log(k) - log(beetles$beetles[1]) + log(beetles$beetles[1] + (k-beetles$beetles[1]) * exp(-r*beetles$days))^2)/2/sigma2) return(l) } ##&#39; define gradiant of loglikelihood function grad.my &lt;- function(para) { k &lt;- para[1] r &lt;- para[2] sigma2 &lt;- para[3] g &lt;- rep(0, 3) g[1] &lt;- sum(-2*(log(beetles$beetles)-log(k)-log(beetles$beetles[1])+ log(beetles$beetles[1]+(k-beetles$beetles[1])* exp(-r*beetles$days)))* (-1/k+exp(-r*beetles$days)/ (beetles$beetles[1]+ (k-beetles$beetles[1])*exp(-r*beetles$days)))/2/sigma2) g[2] &lt;- sum(2*(log(beetles$beetles)-log(k)-log(beetles$beetles[1])+ log(beetles$beetles[1]+(k-beetles$beetles[1])* exp(-r*beetles$days)))* beetles$days*(k-beetles$beetles[1])*exp(-r*beetles$days)/ (beetles$beetles[1]+(k-beetles$beetles[1])*exp(-r*beetles$days))/ 2/sigma2) g[3] &lt;- sum(-1/2/sigma2+(log(beetles$beetles)-log(k)-log(beetles$beetles[1])+ log(beetles$beetles[1]+(k-beetles$beetles[1])* exp(-r*beetles$days)))^2/2/sigma2^2) return(g) } fit &lt;- constrOptim(theta = c(10, 0.1, 1), f = loglikeli, grad = grad.my, ui = diag(1, 3), ci = rep(0, 3), control = list(fnscale = -1), hessian = TRUE) fit$par ## [1] 103.983292 12.124656 2.913935 fit$convergence ## [1] 0 Diag(-solve(fit$hessian)) ## [1] 2.707176e+03 1.212466e+05 2.841633e+00 Using BFGS methods, set constrain to make \\(k\\), \\(r\\), \\(sigma^2\\) nonnegative, we get the MLE estimates \\(\\hat{k} = 103.98\\), \\(\\hat{r} = 12.12\\), \\(\\hat{\\sigma}^2 = 2.91\\). Using inverse of negative hessian, we have these estimates’ variance to be \\(2.7\\times 10^3\\), \\(1.2\\times 10^5\\), \\(2.8\\) respectively. But there’s problem that results will change dramatically with initial values. I just tried several different initial values, get the estimates and compare their corresponding loglikelihood. I just choose the one with maximum loglikelihood. "],
["exercise-4-8-1.html", "Problem 4 Exercise 4.8.1 4.1 E/M-Step Derivations 4.2 A Function to Implement EM Algorithm 4.3 Data Generation and Parameters Estimation", " Problem 4 Exercise 4.8.1 4.1 E/M-Step Derivations \\[\\begin{align} Q(\\Psi|\\Psi^{(k)}) &amp; =\\sum_{Z}\\left[p(Z|\\mathbf{y}, X, \\Psi^{(k)})\\log p(\\mathbf{y}, Z|X, \\Psi)\\right]\\\\ &amp; =\\sum_{Z}\\left[p(Z|\\mathbf{y}, X, \\Psi^{(k)})\\log\\prod^{n}_{i=1}p(y_{i}, \\mathbf{z}_{i}|\\mathbf{x}_{i}, \\Psi)\\right]\\\\ &amp; =\\sum^{n}_{i=1}\\sum_{Z}\\left[p(Z|\\mathbf{y}, X, \\Psi^{(k)})\\log p(y_{i}, \\mathbf{z}_{i}|\\mathbf{x}_{i}, \\Psi)\\right]\\\\ &amp; =\\sum^{n}_{i=1}\\sum_{\\mathbf{z}_{i}}\\left[p(\\mathbf{z}_{i}|\\mathbf{y}, X, \\Psi^{(k)})\\log p(y_{i}, \\mathbf{z}_{i}|\\mathbf{x}_{i}, \\Psi)\\right]\\\\ &amp; =\\sum^{n}_{i=1}\\sum_{\\mathbf{z}_{i}}\\left[p(\\mathbf{z}_{i}|y_{i}, \\mathbf{x}_{i}, \\Psi^{(k)})\\log p(y_{i}, \\mathbf{z}_{i}|\\mathbf{x}_{i}, \\Psi)\\right]\\\\ &amp; =\\sum^{n}_{i=1}\\sum^{m}_{j=1}\\left[p(\\mathbf{z}_{i}=(0,\\cdots,1,\\cdots,0)&#39;|y_{i}, \\mathbf{x}_{i}, \\Psi^{(k)})\\log p(y_{i}, \\mathbf{z}_{i}=(0,\\cdots,1,\\cdots,0)&#39;|\\mathbf{x}_{i}, \\Psi)\\right]\\\\ &amp; =\\sum^{n}_{i=1}\\sum^{m}_{j=1}\\left[p(z_{ij}=1|y_{i}, \\mathbf{x}_{i}, \\Psi^{(k)})\\log p(y_{i}, \\mathbf{z}_{i}=(0,\\cdots,1,\\cdots,0)&#39;|\\mathbf{x}_{i}, \\Psi)\\right]\\\\ &amp; =\\sum^{n}_{i=1}\\sum^{m}_{j=1}\\left[E(z_{ij}|y_{i}, \\mathbf{x}_{i}, \\Psi^{(k)})\\{\\log\\pi_{j}+\\log\\varphi(y_{i}-\\mathbf{x}^{T}_{i}\\boldsymbol{\\beta}_{j}, 0, \\sigma^{2})\\}\\right]\\\\ &amp; =\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\{\\log\\pi_{j}+\\log\\varphi(y_{i}-\\mathbf{x}^{T}_{i}\\boldsymbol{\\beta}_{j}, 0, \\sigma^{2})\\} \\end{align}\\] where \\[\\mathbf{y}=(y_{1},\\cdots,y_{n})&#39;\\] \\[Z= \\begin{pmatrix} \\mathbf{z}_{1}&#39;\\\\ \\vdots\\\\ \\mathbf{z}_{n}&#39; \\end{pmatrix}= \\begin{pmatrix} z_{11} &amp; z_{12} &amp; \\cdots &amp; z_{1m}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ z_{n1} &amp; z_{n2} &amp; \\cdots &amp; z_{nm}\\\\ \\end{pmatrix} \\quad X= \\begin{pmatrix} \\mathbf{x}_{1}&#39;\\\\ \\vdots\\\\ \\mathbf{x}_{n}&#39; \\end{pmatrix}= \\begin{pmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np}\\\\ \\end{pmatrix}\\] \\[p^{(k)}_{ij}=E(z_{ij}|y_{i}, \\mathbf{x}_{i}, \\Psi^{(k)})=\\frac{\\pi^{(k)}_{j}\\varphi(y_{i}-\\mathbf{x}^{T}_{i}\\boldsymbol{\\beta}^{(k)}_{j}, 0, \\sigma^{2^{(k)}})}{\\sum^{m}_{j=1}\\pi^{(k)}_{j}\\varphi(y_{i}-\\mathbf{x}^{T}_{i}\\boldsymbol{\\beta}^{(k)}_{j}, 0, \\sigma^{2^{(k)}})}\\] The elaboration of the above steps are For M-step, since we have \\[\\begin{align*} Q(\\Psi|\\Psi^{(k)}) &amp; =\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\{\\log\\pi_{j}+\\log\\varphi(y_{i}-\\mathbf{x}^{T}_{i}\\boldsymbol{\\beta}_{j}, 0, \\sigma^{2})\\}\\\\ &amp; =\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\log\\pi_{j}-\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\log\\sqrt{2\\pi}\\sigma-\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\frac{(y_{i}-\\mathbf{x}_{i}^{T}\\boldsymbol{\\beta}_{j})^{2}}{2\\sigma^{2}}\\\\ &amp; =\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\log\\frac{\\pi_{j}}{\\sqrt{2\\pi}}-\\frac{1}{2}\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\log\\sigma^{2}-\\frac{1}{2}\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\frac{(y_{i}-\\mathbf{x}_{i}^{T}\\boldsymbol{\\beta}_{j})^{2}}{\\sigma^{2}}\\\\ &amp; =I_{1}-\\frac{1}{2}I_{2}-\\frac{1}{2}I_{3} \\end{align*}\\] From the above, we can see only \\(I_{3}\\) contains \\(\\boldsymbol{\\beta}_{j}\\) and \\[I_{3}=\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\frac{(y_{i}-\\mathbf{x}_{i}^{T}\\boldsymbol{\\beta}_{j})^{2}}{\\sigma^{2}}=\\sum^{m}_{j=1}\\sum^{n}_{i=1}p^{(k)}_{ij}\\frac{(y_{i}-\\mathbf{x}_{i}^{T}\\boldsymbol{\\beta}_{j})^{2}}{\\sigma^{2}}\\] To minimize \\(I_{3}\\), we only need to fix \\(j\\) and optimize with regard to \\(\\boldsymbol{\\beta}_{j}\\). We can directly use the formula from generazied least square method and obtain \\[\\boldsymbol{\\beta}_{j}^{(k+1)}=(X&#39;V^{-1}X)^{-1}X&#39;V^{-1}\\mathbf{y}=\\left(\\sum^{n}_{i=1}\\mathbf{x}_{i}\\mathbf{x}^{T}_{i}p^{(k)}_{ij}\\right)^{-1}\\left(\\sum^{n}_{i=1}\\mathbf{x}_{i}p^{(k)}_{ij}y_{i}\\right)\\quad j=1,\\cdots, m\\] where \\[V^{-1}=diag(p_{1j}^{(k)},\\cdots, p_{nj}^{(k)})\\] Only \\(I_{2}\\) and \\(I_{3}\\) contains \\(\\sigma^{2}\\), since \\[I_{2}+I_{3}=\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\log\\sigma^{2}+\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\frac{(y_{i}-\\mathbf{x}_{i}^{T}\\boldsymbol{\\beta}_{j})^{2}}{\\sigma^{2}}\\] We minimize it with regard to \\(\\sigma^{2}\\) given \\(\\boldsymbol{\\beta}_{j}=\\boldsymbol{\\beta}^{(k+1)}_{j}\\) and obtain \\[\\sigma^{2^{(k+1)}}=\\frac{\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}(y_{i}-\\mathbf{x}_{i}^{T}\\boldsymbol{\\beta}^{(k+1)}_{j})^{2}}{\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}}=\\frac{\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}(y_{i}-\\mathbf{x}_{i}^{T}\\boldsymbol{\\beta}^{(k+1)}_{j})^{2}}{n}\\] Only \\(I_{1}\\) contains \\(\\pi_{j}\\) and \\[I_{1}=\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}\\log\\frac{\\pi_{j}}{\\sqrt{2\\pi}}=-\\frac{1}{2}\\log(2\\pi)\\sum^{n}_{i=1}\\sum^{m}_{j=1}p^{(k)}_{ij}+\\sum^{m}_{j=1}\\left(\\sum^{n}_{i=1}p^{(k)}_{ij}\\right)\\log\\pi_{j}\\] In order to maximize \\(I_{1}\\) under constraint \\(\\pi_{1}+\\cdots\\pi_{m}=1\\). We use Lagrange multiplier method \\[L(\\pi_{1},\\cdots,\\pi_{m})=\\sum^{m}_{j=1}\\left(\\sum^{n}_{i=1}p^{(k)}_{ij}\\right)\\log\\pi_{j}-\\lambda\\left(\\sum^{m}_{j=1}\\pi_{j}-1\\right)\\] with \\(\\lambda\\) a Lagrange multiplier. We can obtain \\[\\pi^{(k+1)}_{j}=\\frac{\\sum^{n}_{i=1}p^{(k)}_{ij}}{\\sum^{m}_{j=1}\\sum^{n}_{i=1}p^{(k)}_{ij}}=\\frac{\\sum^{n}_{i=1}p^{(k)}_{ij}}{n}\\quad j=1,\\cdots,m\\] 4.2 A Function to Implement EM Algorithm regmix_em &lt;- function(y, xmat, pi.init, beta.init, sigma.init, control = list(maxiter = 100, tol = .Machine$double.eps^0.2)){ xmat &lt;- as.matrix(xmat) n &lt;- nrow(xmat) p &lt;- ncol(xmat) m &lt;- length(pi.init) pi &lt;- pi.init beta &lt;- beta.init sigma &lt;- sigma.init maxiter &lt;- control$maxiter tol &lt;- control$tol conv &lt;- 1 P &lt;- matrix(NA, nrow = n, ncol = m) beta.new &lt;- matrix(NA, nrow = p, ncol = m) for (i in 1:maxiter) { for (j in 1:n) { P[j,] &lt;- pi * dnorm(y[j] - xmat[j,] %*% beta, 0, sigma)/ sum(pi * dnorm(y[j] - xmat[j,] %*% beta, 0, sigma)) } pi.new &lt;- apply(P, MARGIN = 2, mean) for (j in 1:m) { beta.new[,j] &lt;- solve(t(xmat) %*% diag(P[,j]) %*% xmat) %*% t(xmat) %*% diag(P[,j]) %*% y } sigma.new &lt;- sqrt(sum(P * (y %*% t(rep(1, m)) - xmat %*% beta.new)^2)/n) conv &lt;- sum(abs(pi.new - pi)) + sum(abs(beta.new - beta)) + abs(sigma.new - sigma) if(conv &lt; tol) break pi &lt;- pi.new beta &lt;- beta.new sigma &lt;- sigma.new } if(i == maxiter) message(&quot;Reached the maximum iteration!&quot;) list(pi = pi.new, beta = beta.new, sigma = sigma.new, conv = conv, iter = i) } 4.3 Data Generation and Parameters Estimation After I carried out the following code, I found parameters won’t be updated after the second iteration. Tracing back to E-Step Derivation, we can see if \\(\\boldsymbol{\\beta}_{1}=\\cdots=\\boldsymbol{\\beta}_{m}\\), then \\(\\mathbf{p}^{(k)}_{\\cdot j}\\) and \\(\\pi^{(k)}_{j}\\) will remain the same at all times. regmix_sim &lt;- function(n, pi, beta, sigma) { K &lt;- ncol(beta) p &lt;- NROW(beta) xmat &lt;- matrix(rnorm(n * p), n, p) # normal covaraites error &lt;- matrix(rnorm(n * K, sd = sigma), n, K) ymat &lt;- xmat %*% beta + error # n by K matrix ind &lt;- t(rmultinom(n, size = 1, prob = pi)) y &lt;- rowSums(ymat * ind) data.frame(y, xmat) } n &lt;- 400 pi &lt;- c(.3, .4, .3) beta &lt;- matrix(c( 1, 1, 1, -1, -1, -1), 2, 3) sigma &lt;- 1 set.seed(1205) dat &lt;- regmix_sim(n, pi, beta, sigma) fit &lt;- regmix_em(y = dat[,1], xmat = dat[,-1], pi.init = pi / pi / length(pi), beta.init = beta * 0, sigma.init = sigma / sigma, control = list(maxiter = 500, tol = 1e-5)) fit ## $pi ## [1] 0.3333333 0.3333333 0.3333333 ## ## $beta ## [,1] [,2] [,3] ## [1,] 0.3335660 0.3335660 0.3335660 ## [2,] -0.4754645 -0.4754645 -0.4754645 ## ## $sigma ## [1] 1.732492 ## ## $conv ## [1] 0 ## ## $iter ## [1] 2 Thus we change the initial values of \\(\\boldsymbol{\\beta}_{1},\\cdots,\\boldsymbol{\\beta}_{m}\\). And we can see this time after \\(83\\) iterations, the algorithm converged and I got the following consequences. fit1 &lt;- regmix_em(y = dat[,1], xmat = dat[,-1], pi.init = pi / pi / length(pi), beta.init = matrix(1:6, 2, 3), sigma.init = sigma / sigma, control = list(maxiter = 500, tol = 1e-5)) fit1 ## $pi ## [1] 0.3454017 0.3858262 0.2687721 ## ## $beta ## [,1] [,2] [,3] ## [1,] -0.9136801 0.8796636 0.9912061 ## [2,] -1.1990374 0.9341887 -1.2424685 ## ## $sigma ## [1] 1.023598 ## ## $conv ## [1] 9.786183e-06 ## ## $iter ## [1] 83 "]
]
